# -*- coding: utf-8 -*-
"""Bruhma.AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/174dGETbBuyzBGnH4covPcf4ENWZ_S41O
"""

# THE BRUHMA.AI is an adavnced artificial intelligence based on langchain and Tinyllama(trained on 187B parameters)

!pip install gradio google beautifulsoup4
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from typing import List, Dict
import gc
from googlesearch import search
import requests
from bs4 import BeautifulSoup

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LENGTH = 512
TEMPERATURE = 0.7

custom_css = """
/* Modern Dark Theme */
:root {
    --primary: #2b313e;
    --secondary: #3a4150;
    --accent: #6875f5;
    --text: #ffffff;
    --border: 1px solid #4a5568;
}

body {
    background: var(--primary);
    font-family: 'Inter', sans-serif;
}

/* Chat Container */
.contain {
    background: linear-gradient(160deg, var(--primary) 0%, #1a202c 100%);
    border-radius: 16px;
    box-shadow: 0 4px 30px rgba(0, 0, 0, 0.3);
}

/* Chat Messages */
.message {
    padding: 16px 24px;
    margin: 12px 0;
    border-radius: 20px;
    backdrop-filter: blur(10px);
}

.user-message {
    background: rgba(39, 55, 77, 0.9);
    border: 1px solid #4a5568;
}

.bot-message {
    background: rgba(26, 32, 44, 0.9);
    border: 1px solid #4a5568;
}

/* Inputs */
textarea, input {
    background: var(--secondary) !important;
    color: var(--text) !important;
    border-radius: 12px !important;
    padding: 14px !important;
}

/* Buttons */
button {
    background: var(--accent) !important;
    color: white !important;
    border-radius: 12px !important;
    padding: 12px 24px !important;
    transition: all 0.3s ease !important;
}

button:hover {
    transform: translateY(-2px);
    box-shadow: 0 5px 15px rgba(104, 117, 245, 0.4);
}

/* Slider */
.rc-slider-handle {
    border: solid 2px var(--accent) !important;
}

.dark .rc-slider-track {
    background-color: var(--accent) !important;
}
"""

def is_greeting(user_input: str) -> bool:
    greetings = ["hi", "hello", "hey", "greetings", "good morning", "good afternoon", "good evening"]
    input_clean = user_input.strip().lower()
    return any(input_clean == g or input_clean.startswith(g + " ") for g in greetings)

class AdvancedChatBot:
    def __init__(self):
        print("Loading model...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            low_cpu_mem_usage=True,
            offload_folder="offload"
        )
        self.chat_history = []
        self.current_system_prompt = ""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("Model loaded successfully!")

    def generate_response(self, user_input: str, system_prompt: str = "",
                         temperature: float = TEMPERATURE) -> str:
        try:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            if system_prompt != self.current_system_prompt:
                self.current_system_prompt = system_prompt
                self.chat_history = []

            conversation = self._format_conversation(user_input, system_prompt)
            inputs = self.tokenizer(
                conversation,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(DEVICE)

            with torch.no_grad():
                outputs = self.model.generate(
                    inputs["input_ids"],
                    max_new_tokens=768,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.85,
                    top_k=40,
                    num_beams=1,
                    num_return_sequences=1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    repetition_penalty=1.15,
                    early_stopping=True
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.split("<|im_start|>assistant")[-1].strip()
            response = response.replace("<|im_end|>", "").strip()

            del inputs, outputs
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

            if len(self.chat_history) > 12:
                self.chat_history = self.chat_history[-12:]

            self.chat_history.append({"role": "user", "content": user_input})
            self.chat_history.append({"role": "assistant", "content": response})

            return response

        except Exception as e:
            print(f"Error: {str(e)}")
            return "I apologize, but I encountered an error. Please try again."

    def _format_conversation(self, user_input: str, system_prompt: str) -> str:
        default_prompt = """You are a helpful AI assistant. Follow these rules:
1. Maintain your specified character/persona at all times
2. Provide detailed, accurate responses
3. Keep conversations natural and engaging
4. Always obey the system instructions
"""
        conversation_header = system_prompt if system_prompt else default_prompt
        system_msg = f"<|im_start|>system\n{conversation_header}<|im_end|>\n"
        conversation = [system_msg]

        for message in self.chat_history[-6:]:
            role = message['role']
            content = message['content']
            conversation.append(f"<|im_start|>{role}\n{content}<|im_end|>\n")

        conversation.append(f"<|im_start|>user\n{user_input}<|im_end|>\n")
        conversation.append("<|im_start|>assistant\n")

        return "".join(conversation)

    def clear_history(self):
        self.chat_history = []
        self.current_system_prompt = ""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

chatbot = AdvancedChatBot()

def chat_interface_wrapper(message: str, history: List[Dict], system_prompt: str, temperature: float) -> List[Dict]:
    try:
        if history is None:
            history = []

        history.append({"role": "user", "content": message})

        if is_greeting(message):
            greeting_response = "Hello there! How can I help you today?"
            history.append({"role": "assistant", "content": greeting_response})
            return history

        response = chatbot.generate_response(
            user_input=message,
            system_prompt=system_prompt,
            temperature=temperature
        )

        history.append({"role": "assistant", "content": response})
        return history

    except Exception as e:
        print(f"Interface error: {str(e)}")
        history.append({"role": "assistant", "content": "I encountered an error. Please try again."})
        return history

def clear_chat_history():
    chatbot.clear_history()
    return []

def web_search(query: str, search_depth: int, search_timeout: int) -> str:
    try:
        search_results = list(search(query, stop=search_depth))
        if not search_results:
            return "No search results found."

        summaries = []
        for url in search_results[:search_depth]:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=search_timeout)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')
                for script in soup(["script", "style"]):
                    script.decompose()

                text = ' '.join([p.get_text().strip() for p in soup.find_all(['p','h1','h2','h3','div'])])
                text = ' '.join(text.split())

                if text:
                    summary = text[:1000] + "..." if len(text) > 1000 else text
                    summaries.append(f"üåê Result from {url}:\n{summary}\n")

            except Exception:
                continue

        if not summaries:
            return "Could not extract meaningful content from the search results."
        return "\n\n".join(summaries[:search_depth])

    except Exception as e:
        return f"Sorry, couldn't perform web search: {str(e)}"

def handle_web_search(message: str, history: List[Dict], system_prompt: str,
                      temperature: float, search_depth: int, search_timeout: int) -> List[Dict]:
    if history is None:
        history = []

    history.append({"role": "user", "content": message})
    search_result = web_search(message, search_depth, search_timeout)
    combined_input = f"Web search results for '{message}':\n{search_result}"

    response = chatbot.generate_response(
        user_input=combined_input,
        system_prompt=system_prompt,
        temperature=temperature
    )

    history.append({"role": "assistant", "content": response})
    return history

footer = """
<div style="position: fixed; bottom: 10px; right: 10px; display: flex; align-items: center; gap: 8px; font-size: 0.8em; color: #718096;">
    <span>Built and maintained by Shreyas Chowdhury</span>
    <br><a href="mailto:chowdhuryshreyas740@gmail.com" style="display: flex; align-items: center; gap: 4px; text-decoration: none; color: #a0aec0;">
        <img src="https://img.icons8.com/ios-glyphs/30/FFFFFF/gmail.png" style="width: 16px; height: 16px;">
        Contact Us
    </a>
</div>
"""

with gr.Blocks(css=custom_css, theme=gr.themes.Default(primary_hue="blue")) as demo:
    with gr.Column(elem_id="main-container"):
        gr.HTML("""
<div class="header">
    <div style="display: flex; align-items: center; gap: 12px; margin-bottom: 0;">
        <img src="https://cdn-icons-png.flaticon.com/512/4712/4712139.png"
             style="width: 32px; height: 32px; object-fit: contain;">
        <h1 style="color: #6875f5; font-weight: 800; margin-bottom: 0;">Bruhma.AI</h1>
    </div>
    <p style="color: #a0aec0; margin-top: 8px;">Advanced Conversational AI with Web Integration</p>
</div>
""")

        with gr.Row():
            with gr.Column(scale=3):
                chatbot_interface = gr.Chatbot(
                    elem_id="chat-container",
                    bubble_full_width=False,
                    avatar_images=(
                        "https://cdn-icons-png.flaticon.com/512/1494/1494851.png",
                        "https://cdn-icons-png.flaticon.com/512/4712/4712035.png"
                    ),
                    height=600,
                    type="messages"
                )

                with gr.Row():
                    msg = gr.Textbox(
                        placeholder="Type your message...",
                        show_label=False,
                        container=False,
                        scale=5,
                        autofocus=True
                    )
                    submit = gr.Button("Send", variant="primary", scale=1)

                with gr.Row():
                    web_search_btn = gr.Button("üåê Web Search", variant="secondary")
                    new_chat_btn = gr.Button("üìù New Chat", variant="secondary")

            with gr.Column(scale=1):
                with gr.Accordion("AI Configuration", open=True):
                    gr.Markdown("### Model Settings")
                    temperature_slider = gr.Slider(
                        0.1, 1.0, value=0.7, step=0.05, label="Creativity"
                    )
                    system_prompt_box = gr.Textbox(
                        label="Custom Behavior",
                        placeholder="Example: 'You are a pirate. Use phrases like 'Arrr!' and 'Ahoy!'",
                        lines=3
                    )

                with gr.Accordion("Web Search Settings", open=False):
                    search_depth_radio = gr.Radio([3, 5, 7], value=3, label="Search Depth")
                    search_timeout_box = gr.Number(5, label="Timeout (seconds)")

                gr.Markdown("""
                ### Quick Tips
                - Press Shift+Enter for a new line
                - Character personas work best with 1-2 sentence descriptions
                - Use 'New Chat' to reset behavior
                """)

        # Add the footer here
        gr.HTML(footer)

    submit.click(
        fn=chat_interface_wrapper,
        inputs=[msg, chatbot_interface, system_prompt_box, temperature_slider],
        outputs=[chatbot_interface]
    ).then(
        fn=lambda: "",
        outputs=[msg]
    )

    msg.submit(
        fn=chat_interface_wrapper,
        inputs=[msg, chatbot_interface, system_prompt_box, temperature_slider],
        outputs=[chatbot_interface]
    ).then(
        fn=lambda: "",
        outputs=[msg]
    )

    new_chat_btn.click(
        fn=clear_chat_history,
        outputs=[chatbot_interface]
    )

    web_search_btn.click(
        fn=handle_web_search,
        inputs=[
            msg,
            chatbot_interface,
            system_prompt_box,
            temperature_slider,
            search_depth_radio,
            search_timeout_box
        ],
        outputs=[chatbot_interface]
    ).then(
        fn=lambda: "",
        outputs=[msg]
    )

if __name__ == "__main__":
    print("Testing model...")
    try:
        test_prompt = "You are a pirate captain. Use pirate slang and nautical terms."
        test_response = chatbot.generate_response(
            "What's the weather forecast?",
            system_prompt=test_prompt
        )
        print(f"Pirate test response: {test_response}")

        follow_up = chatbot.generate_response(
            "Tell me more about tropical storms in the Caribbean",
            system_prompt=test_prompt
        )
        print(f"Follow-up response length: {len(follow_up)} characters")

    except Exception as e:
        print(f"Test failed: {str(e)}")

    print("Launching interface...")
    demo.launch(share=True)

